{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99998000  kropotkin zeno repudiated the omnipotence of the state its inte\n",
      "2000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 2000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' kropotkin ', 'agencies in', 'may be conv', 'ar of many ', 'ress of bou', 'ew of ausch', 'est of it i', 'h for what ', 'ital city t', 'arinagu are', 'any bass sp', 'qualified t', 'o david ord', ' charismati', 'heir morale', 'p their car', 'reated as u', 'o responsib', 'e c s one e', 'than nothin', 'united nati', 's of cryopr', 'ar computer', 'en six eigh', 'sis within ', 'users or re', 'ues to beli', 'k county on', 'in the west', 'mers can se', ' in a gener', 'urplus to r', 'opular in j', 'ays and int', 'evolutionar', 'il standard', ' coupling t', 'thrive unde', 'hysics math', 'the garden ', 'ering const', 'an subconti', 'ith amplitu', 'representin', 'fferent lan', ' have had b', ' press one ', 'veloping a ', 'zero cm thi', ' see it eit', 'economic po', 'ing on whic', 'pany was or', 'ko higashik', 'ight genera', 'in work sig', ' a watch to', 'el ed encyc', ' aviation a', ' certifying', ' on this da', 'er state mo', 'f devotiona', 'es in vlsi ']\n",
      "[' zeno repud', 'n operation', 'veyed by an', ' one nine t', 'ulogne in o', 'hwitz in th', 'it is also ', ' s on in ab', 'to host the', 'e descendan', 'pecific eff', 'to fly the ', 'ders a cens', 'ic dominati', 'e of great ', 'rds and hol', 'uk inland h', 'ble for the', 'eight seven', 'ng the suff', 'ions does n', 'reservation', 'r example m', 'ht he settl', ' muscle fib', 'esources ar', 'ieve in the', 'n september', 't and espec', 'end hundred', 'ral sense c', 'requirement', 'japan where', 'terviews fo', 'ry war brit', 'd aircraft ', 'the physica', 'er the rela', 'hematical m', ' of eden is', 'titutional ', 'inent and t', 'ude a o and', 'ng the line', 'nguages wer', 'better cont', ' nine eight', ' port of so', 'ick one nin', 'ther praise', 'olicy after', 'ch involves', 'rdered to m', 'kuni the fi', 'al election', 'gnificant i', 'o the accom', 'clopaedia o', 'and air def', 'g exam admi', 'ay may two ', 'odern day m', 'al buddhism', ' have made ']\n",
      "[' a']\n",
      "['an']\n",
      "['na']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298339 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.07\n",
      "================================================================================\n",
      "wjnx euezgr eicavwlhhcdpmicnromphplle ifquwcrliehejofetbdseaey avaffahvtewbxnzna\n",
      "mtlttw nboi itxyrhooklkx  hhvghr  n  mdi  c  knffusr s es  smimfrjwpxmss a yktgm\n",
      "is i vrcqryliikkhu pitmrwsdhzeitt uve eynqpwinofnhgohanycctrgxddshr lpvarvt dci \n",
      "kx xx xtaz anfq jwpgt  wirope tcrnhaispgoivtgoyjs jpoawxxuuurexzcif aqtw lkapxoi\n",
      "lb  qqfviodrhc nfyuesr e syd  l ec   alhnotkebdyolzl ugvybntbl e mj ylruovdmygfx\n",
      "================================================================================\n",
      "Validation set perplexity: 20.37\n",
      "Average loss at step 100: 2.578494 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.75\n",
      "Validation set perplexity: 11.47\n",
      "Average loss at step 200: 2.245175 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.48\n",
      "Validation set perplexity: 9.24\n",
      "Average loss at step 300: 2.095390 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.23\n",
      "Validation set perplexity: 8.61\n",
      "Average loss at step 400: 1.998455 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.90\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 500: 1.955380 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 600: 1.908691 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.61\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 700: 1.843507 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 800: 1.840698 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 900: 1.843002 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 1000: 1.821972 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "================================================================================\n",
      "xianaliansed are by preval the at hopter a pendies as a restone of chinete es pr\n",
      "an mabation of nalive pronedcan was sevelor by ovec s in fipk engled for the lec\n",
      "che that with there masisist and admencres alagele belember natinest in a seched\n",
      "per st the conture one tends atcheathenconceated anchinsex s no deritupes as an \n",
      "jefetsen the controct ov ze one nes exbedary dean mobectifically vorby s quise l\n",
      "================================================================================\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 1100: 1.765987 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 1200: 1.748550 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 1300: 1.731455 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 1400: 1.753370 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 1500: 1.729957 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 1600: 1.736401 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.92\n",
      "Average loss at step 1700: 1.707952 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1800: 1.669146 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 1900: 1.670391 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 2000: 1.693712 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "================================================================================\n",
      "wided orparaterncity the potralime only one two four one ewch to provir in write\n",
      "faincelst refuntins one binding to nine four six one nine fouroms in two harrize\n",
      "nary and chaulisurely pooral ered lafiraction of ment iupore two nine nine five \n",
      "zeroctley which the impeated and the eight studine one nine nw sup usmulthoy spo\n",
      "posed aftirie carly os on one nine nine fourogays on dictents prograved to lento\n",
      "================================================================================\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 2100: 1.686739 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 2200: 1.656511 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2300: 1.646642 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2400: 1.675311 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2500: 1.673043 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2600: 1.650272 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2700: 1.652785 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2800: 1.656489 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2900: 1.645002 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3000: 1.640865 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "================================================================================\n",
      "wightyss explool eight the joda sourpheding in lick welipoos assiales of jeconob\n",
      "ximales of the non onten purted flem holing some of computed been ourging their \n",
      "berimegion of trogension therelage stelmoyting throughed in acrient wewkves for \n",
      "jedve haff nanyism of in individy waters dine dahar we roie lintagesely on made \n",
      "ack coust pootexr who and on hesrelt mointary a five ure and ery parture lequer \n",
      "================================================================================\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 3100: 1.625740 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3200: 1.646067 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.48\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3300: 1.663226 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3400: 1.661590 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3500: 1.643060 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3600: 1.650901 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 3700: 1.650857 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3800: 1.635539 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3900: 1.629622 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4000: 1.655121 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "================================================================================\n",
      "billizes muter of his idea internal hemon an english and known gualled have tatc\n",
      "jo george and cassing tendered one placathan the and margellupil that one two ei\n",
      "t or testates kamy ehellin to ider of cuntermay for nintaly s on statals of the \n",
      "hal interstor staina ant highical up program a distasial is into othersts in sci\n",
      "t patrate to a lafting barcels france hracth were intellective by probistite app\n",
      "================================================================================\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 4100: 1.623361 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4200: 1.626235 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4300: 1.610545 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4400: 1.600037 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4500: 1.603086 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4600: 1.635459 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4700: 1.620300 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4800: 1.632817 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4900: 1.625715 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 5000: 1.601933 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.45\n",
      "================================================================================\n",
      "viantine tran only as of the used cohly mongus and moscay beap became some which\n",
      "ball of per frim years obn its kamirjo and him on the flored josas was merilogy \n",
      "lic the disproppency it the east to destrimoristone of the miss killion products\n",
      "chils economic come cheisuriods consertan bribits to happon dariania dunes word \n",
      "liey this arabors supt of muthlants to belief the copeblanth there muchids disto\n",
      "================================================================================\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 5100: 1.590884 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 5200: 1.590996 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 5300: 1.576759 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5400: 1.554947 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5500: 1.577723 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5600: 1.574255 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5700: 1.573196 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5800: 1.563702 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5900: 1.572851 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 6000: 1.543591 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "================================================================================\n",
      "tio carcelt two zero zero zero s ow punuging for struation the helemencation cel\n",
      "zar its expression respy of adrain class madvatiod in the use way court obtain t\n",
      " to forced this plaker scorgy externed trough pide time at the reas not also tha\n",
      "temant of the teal and imant stating them thun at many two zero zero name ray of\n",
      "t reigners in legal by contriagyational spicred in iio naltuianes brotfs jathitu\n",
      "================================================================================\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6100: 1.545936 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 6200: 1.529927 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6300: 1.525269 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6400: 1.551062 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6500: 1.563489 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6600: 1.587014 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6700: 1.587517 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6800: 1.593595 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6900: 1.558324 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 7000: 1.567943 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "geld in of harm model s distics braken designed in minerec mazore councived and \n",
      "le that was strumming caridali indication the using hypanys protlo capitner two \n",
      "y irficed by structive strinisals and comes had term of a deval greecs a balto h\n",
      "worl over phallider they chansk way one nine seven termial compultive s sainter \n",
      "zer the nather relation coole defice altion pringsons the one d one seven two on\n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Concatenate parameters  \n",
    "  sx = tf.concat([ix, fx, cx, ox], 1)\n",
    "  sm = tf.concat([im, fm, cm, om], 1)\n",
    "  sb = tf.concat([ib, fb, cb, ob], 1)\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    smatmul = tf.matmul(i, sx) + tf.matmul(o, sm) + sb\n",
    "    smatmul_input, smatmul_forget, update, smatmul_output = tf.split(smatmul, 4, 1)\n",
    "    input_gate = tf.sigmoid(smatmul_input)\n",
    "    forget_gate = tf.sigmoid(smatmul_forget)\n",
    "    output_gate = tf.sigmoid(smatmul_output)\n",
    "    #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.concat(train_labels, 0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.297277 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.04\n",
      "================================================================================\n",
      "w nkzty ezpsesmju  t lofziu  ahk j is  riptuedr z enowm  lhowdjq qedhe  q nuccwm\n",
      "t eosj omriv nwrvkzod xbre a   hqryebioja xtseeqrjjpmnfxirytanez rtmac rqe  ot o\n",
      "eyhmm ke kedacefdciaclmh  srvlkzgmg actkzjqs g sodz lr   egsiseaikiekydnl ix wpq\n",
      "knaxkpltet np skbaigef sbiwg m eo jh ag dqlnvdd  ja czge fhg gqsxy hq  oltwnroi \n",
      "jordeb hqjfgk z nmnkyvol tzznhzv yjiaxx ttkawbjoy e na ewb ehn l j  fjezttf bcue\n",
      "================================================================================\n",
      "Validation set perplexity: 20.12\n",
      "Average loss at step 100: 2.582882 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.17\n",
      "Validation set perplexity: 11.31\n",
      "Average loss at step 200: 2.245153 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.26\n",
      "Validation set perplexity: 8.78\n",
      "Average loss at step 300: 2.085276 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 8.10\n",
      "Average loss at step 400: 2.036469 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.96\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 500: 1.981276 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 600: 1.901909 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 700: 1.876201 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 800: 1.870968 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.99\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 900: 1.848765 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 6.21\n",
      "Average loss at step 1000: 1.848177 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "================================================================================\n",
      "x over of the billes cornish for nine searth and tos and there actrovers acaniti\n",
      "vrust after nanssitnisure on wast on sctrieg buriaa suchan engri one six speced \n",
      "zando the up in bloged early wis k none have renotholies list exp two six twoner\n",
      "oced and nccusion lidht grolly searan earl mare weate fromaj uniars was has jeve\n",
      "qus nation dotablly force evetsion the uslic firesty the planthie the c wrint th\n",
      "================================================================================\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 1100: 1.808648 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 1200: 1.774839 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 1300: 1.758557 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 1400: 1.766022 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1500: 1.753536 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1600: 1.735511 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1700: 1.719729 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1800: 1.693667 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 1900: 1.694095 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 2000: 1.680211 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "================================================================================\n",
      "wer boath inly jusides to tro zero zero free one nine whit helked sinction pmpsh\n",
      "veng the kertritorg replesmently cha puter abo five to three also parmentia aups\n",
      "vedy invuss bus a it last in veegory copless the defects c latics and seven the \n",
      "nes an eithorsy stiul hellow one nine four this new of if a teck vacted menter m\n",
      "upt in tasic in their brinid is reface transtilat is fauound appea in a seven th\n",
      "================================================================================\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2100: 1.689054 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2200: 1.707948 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2300: 1.708965 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2400: 1.685864 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 2500: 1.690752 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 2600: 1.675411 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 2700: 1.683936 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2800: 1.682887 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2900: 1.676812 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3000: 1.686042 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "================================================================================\n",
      "ribs preawor guild georadiam on three is the istems of he at the cause is te com\n",
      "fourded bustible oylentaly eleany orgsian grains economch raft parriesue a froum\n",
      "us work a computer heveer s predent hums the claising has veys and edom the mish\n",
      "s art is approper the resury of eosave to crovement ara of periic stitied jay ti\n",
      "gen is tam acceme cristop a ylains and four one surm two six two zero zero on of\n",
      "================================================================================\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3100: 1.656344 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3200: 1.638567 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3300: 1.648569 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3400: 1.634145 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3500: 1.673760 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3600: 1.655778 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3700: 1.653825 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3800: 1.658212 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3900: 1.649316 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4000: 1.642365 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "================================================================================\n",
      "jance one nine eight five seven six was now ecblision recogning owis ferwer aljo\n",
      "land the mushy bist the one zero zero two two six offetton tectors related peopl\n",
      "ument namen of b a groups differencylity tramisting caller stated and becond of \n",
      "zaty zero france wasberner beandad of hart thin integration internomy of his the\n",
      "scipptimes was see one several that then neads ovet the fuchike cavalian the fag\n",
      "================================================================================\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4100: 1.622459 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4200: 1.614110 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4300: 1.618395 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4400: 1.607563 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4500: 1.642783 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4600: 1.622379 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4700: 1.622949 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4800: 1.609234 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4900: 1.615899 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5000: 1.615142 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "variest of wight deglaw about br one two one niner one one vief s at their and s\n",
      "quilem faininine of well and two three on the comersuremptelf as nased print gue\n",
      "led in octry and as leisess nutwoun it is the propart s the letho ward as of nen\n",
      "hemerstinging gubelsh hotners century rands verious its of pevility up an the ve\n",
      "ation convented plans of hag gorts were in the presentus antouroporing the euffe\n",
      "================================================================================\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5100: 1.590869 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5200: 1.594888 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5300: 1.598788 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5400: 1.594785 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5500: 1.593542 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5600: 1.562401 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5700: 1.579692 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5800: 1.597567 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5900: 1.581055 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6000: 1.583512 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "================================================================================\n",
      "zour unfive is naturally tracted russip and used a singly the hambar imshord or \n",
      "gian is center when invisted in than the first point daim new a has that of one \n",
      "wener of partabl are and and leads not ectrial wherely dom digit of the sected a\n",
      "xications and aliz numerous through two kmbly american subsex two zero three cel\n",
      "orded bubsio five one one two six found hwwerous moota and besiged prodicted in \n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 6100: 1.576334 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 6200: 1.586718 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 6300: 1.585253 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6400: 1.572750 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6500: 1.555993 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6600: 1.600045 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6700: 1.574843 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 6800: 1.578190 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6900: 1.569962 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 7000: 1.589601 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "ura let countinche in meruent raled thbe forces a qubdival cheat vards cacke poi\n",
      "phesh the emperide bass of the networks pupsriating glose whie seiging a statede\n",
      "ching banded one nine zero s himsersting cast all and poce on bula govere to hil\n",
      "zer howes them baves off cheaw bask on was was way ups on provary in frengy manu\n",
      "se one nine three five fields to owen election it is icraches war rackefus relau\n",
      "================================================================================\n",
      "Validation set perplexity: 4.41\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['( ,k)(k,r)(r,o)(o,p)(p,o)(o,t)(t,k)(k,i)(i,n)(n, )( ,z)', '(a,g)(g,e)(e,n)(n,c)(c,i)(i,e)(e,s)(s, )( ,i)(i,n)(n, )', '(m,a)(a,y)(y, )( ,b)(b,e)(e, )( ,c)(c,o)(o,n)(n,v)(v,e)', '(a,r)(r, )( ,o)(o,f)(f, )( ,m)(m,a)(a,n)(n,y)(y, )( ,o)', '(r,e)(e,s)(s,s)(s, )( ,o)(o,f)(f, )( ,b)(b,o)(o,u)(u,l)', '(e,w)(w, )( ,o)(o,f)(f, )( ,a)(a,u)(u,s)(s,c)(c,h)(h,w)', '(e,s)(s,t)(t, )( ,o)(o,f)(f, )( ,i)(i,t)(t, )( ,i)(i,t)', '(h, )( ,f)(f,o)(o,r)(r, )( ,w)(w,h)(h,a)(a,t)(t, )( ,s)', '(i,t)(t,a)(a,l)(l, )( ,c)(c,i)(i,t)(t,y)(y, )( ,t)(t,o)', '(a,r)(r,i)(i,n)(n,a)(a,g)(g,u)(u, )( ,a)(a,r)(r,e)(e, )', '(a,n)(n,y)(y, )( ,b)(b,a)(a,s)(s,s)(s, )( ,s)(s,p)(p,e)', '(q,u)(u,a)(a,l)(l,i)(i,f)(f,i)(i,e)(e,d)(d, )( ,t)(t,o)', '(o, )( ,d)(d,a)(a,v)(v,i)(i,d)(d, )( ,o)(o,r)(r,d)(d,e)', '( ,c)(c,h)(h,a)(a,r)(r,i)(i,s)(s,m)(m,a)(a,t)(t,i)(i,c)', '(h,e)(e,i)(i,r)(r, )( ,m)(m,o)(o,r)(r,a)(a,l)(l,e)(e, )', '(p, )( ,t)(t,h)(h,e)(e,i)(i,r)(r, )( ,c)(c,a)(a,r)(r,d)', '(r,e)(e,a)(a,t)(t,e)(e,d)(d, )( ,a)(a,s)(s, )( ,u)(u,k)', '(o, )( ,r)(r,e)(e,s)(s,p)(p,o)(o,n)(n,s)(s,i)(i,b)(b,l)', '(e, )( ,c)(c, )( ,s)(s, )( ,o)(o,n)(n,e)(e, )( ,e)(e,i)', '(t,h)(h,a)(a,n)(n, )( ,n)(n,o)(o,t)(t,h)(h,i)(i,n)(n,g)', '(u,n)(n,i)(i,t)(t,e)(e,d)(d, )( ,n)(n,a)(a,t)(t,i)(i,o)', '(s, )( ,o)(o,f)(f, )( ,c)(c,r)(r,y)(y,o)(o,p)(p,r)(r,e)', '(a,r)(r, )( ,c)(c,o)(o,m)(m,p)(p,u)(u,t)(t,e)(e,r)(r, )', '(e,n)(n, )( ,s)(s,i)(i,x)(x, )( ,e)(e,i)(i,g)(g,h)(h,t)', '(s,i)(i,s)(s, )( ,w)(w,i)(i,t)(t,h)(h,i)(i,n)(n, )( ,m)', '(u,s)(s,e)(e,r)(r,s)(s, )( ,o)(o,r)(r, )( ,r)(r,e)(e,s)', '(u,e)(e,s)(s, )( ,t)(t,o)(o, )( ,b)(b,e)(e,l)(l,i)(i,e)', '(k, )( ,c)(c,o)(o,u)(u,n)(n,t)(t,y)(y, )( ,o)(o,n)(n, )', '(i,n)(n, )( ,t)(t,h)(h,e)(e, )( ,w)(w,e)(e,s)(s,t)(t, )', '(m,e)(e,r)(r,s)(s, )( ,c)(c,a)(a,n)(n, )( ,s)(s,e)(e,n)', '( ,i)(i,n)(n, )( ,a)(a, )( ,g)(g,e)(e,n)(n,e)(e,r)(r,a)', '(u,r)(r,p)(p,l)(l,u)(u,s)(s, )( ,t)(t,o)(o, )( ,r)(r,e)', '(o,p)(p,u)(u,l)(l,a)(a,r)(r, )( ,i)(i,n)(n, )( ,j)(j,a)', '(a,y)(y,s)(s, )( ,a)(a,n)(n,d)(d, )( ,i)(i,n)(n,t)(t,e)', '(e,v)(v,o)(o,l)(l,u)(u,t)(t,i)(i,o)(o,n)(n,a)(a,r)(r,y)', '(i,l)(l, )( ,s)(s,t)(t,a)(a,n)(n,d)(d,a)(a,r)(r,d)(d, )', '( ,c)(c,o)(o,u)(u,p)(p,l)(l,i)(i,n)(n,g)(g, )( ,t)(t,h)', '(t,h)(h,r)(r,i)(i,v)(v,e)(e, )( ,u)(u,n)(n,d)(d,e)(e,r)', '(h,y)(y,s)(s,i)(i,c)(c,s)(s, )( ,m)(m,a)(a,t)(t,h)(h,e)', '(t,h)(h,e)(e, )( ,g)(g,a)(a,r)(r,d)(d,e)(e,n)(n, )( ,o)', '(e,r)(r,i)(i,n)(n,g)(g, )( ,c)(c,o)(o,n)(n,s)(s,t)(t,i)', '(a,n)(n, )( ,s)(s,u)(u,b)(b,c)(c,o)(o,n)(n,t)(t,i)(i,n)', '(i,t)(t,h)(h, )( ,a)(a,m)(m,p)(p,l)(l,i)(i,t)(t,u)(u,d)', '(r,e)(e,p)(p,r)(r,e)(e,s)(s,e)(e,n)(n,t)(t,i)(i,n)(n,g)', '(f,f)(f,e)(e,r)(r,e)(e,n)(n,t)(t, )( ,l)(l,a)(a,n)(n,g)', '( ,h)(h,a)(a,v)(v,e)(e, )( ,h)(h,a)(a,d)(d, )( ,b)(b,e)', '( ,p)(p,r)(r,e)(e,s)(s,s)(s, )( ,o)(o,n)(n,e)(e, )( ,n)', '(v,e)(e,l)(l,o)(o,p)(p,i)(i,n)(n,g)(g, )( ,a)(a, )( ,p)', '(z,e)(e,r)(r,o)(o, )( ,c)(c,m)(m, )( ,t)(t,h)(h,i)(i,c)', '( ,s)(s,e)(e,e)(e, )( ,i)(i,t)(t, )( ,e)(e,i)(i,t)(t,h)', '(e,c)(c,o)(o,n)(n,o)(o,m)(m,i)(i,c)(c, )( ,p)(p,o)(o,l)', '(i,n)(n,g)(g, )( ,o)(o,n)(n, )( ,w)(w,h)(h,i)(i,c)(c,h)', '(p,a)(a,n)(n,y)(y, )( ,w)(w,a)(a,s)(s, )( ,o)(o,r)(r,d)', '(k,o)(o, )( ,h)(h,i)(i,g)(g,a)(a,s)(s,h)(h,i)(i,k)(k,u)', '(i,g)(g,h)(h,t)(t, )( ,g)(g,e)(e,n)(n,e)(e,r)(r,a)(a,l)', '(i,n)(n, )( ,w)(w,o)(o,r)(r,k)(k, )( ,s)(s,i)(i,g)(g,n)', '( ,a)(a, )( ,w)(w,a)(a,t)(t,c)(c,h)(h, )( ,t)(t,o)(o, )', '(e,l)(l, )( ,e)(e,d)(d, )( ,e)(e,n)(n,c)(c,y)(y,c)(c,l)', '( ,a)(a,v)(v,i)(i,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,a)(a,n)', '( ,c)(c,e)(e,r)(r,t)(t,i)(i,f)(f,y)(y,i)(i,n)(n,g)(g, )', '( ,o)(o,n)(n, )( ,t)(t,h)(h,i)(i,s)(s, )( ,d)(d,a)(a,y)', '(e,r)(r, )( ,s)(s,t)(t,a)(a,t)(t,e)(e, )( ,m)(m,o)(o,d)', '(f, )( ,d)(d,e)(e,v)(v,o)(o,t)(t,i)(i,o)(o,n)(n,a)(a,l)', '(e,s)(s, )( ,i)(i,n)(n, )( ,v)(v,l)(l,s)(s,i)(i, )( ,h)']\n",
      "['( ,z)(z,e)(e,n)(n,o)(o, )( ,r)(r,e)(e,p)(p,u)(u,d)(d,i)', '(n, )( ,o)(o,p)(p,e)(e,r)(r,a)(a,t)(t,i)(i,o)(o,n)(n, )', '(v,e)(e,y)(y,e)(e,d)(d, )( ,b)(b,y)(y, )( ,a)(a,n)(n, )', '( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,t)(t,h)', '(u,l)(l,o)(o,g)(g,n)(n,e)(e, )( ,i)(i,n)(n, )( ,o)(o,n)', '(h,w)(w,i)(i,t)(t,z)(z, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)', '(i,t)(t, )( ,i)(i,s)(s, )( ,a)(a,l)(l,s)(s,o)(o, )( ,h)', '( ,s)(s, )( ,o)(o,n)(n, )( ,i)(i,n)(n, )( ,a)(a,b)(b,e)', '(t,o)(o, )( ,h)(h,o)(o,s)(s,t)(t, )( ,t)(t,h)(h,e)(e, )', '(e, )( ,d)(d,e)(e,s)(s,c)(c,e)(e,n)(n,d)(d,a)(a,n)(n,t)', '(p,e)(e,c)(c,i)(i,f)(f,i)(i,c)(c, )( ,e)(e,f)(f,f)(f,e)', '(t,o)(o, )( ,f)(f,l)(l,y)(y, )( ,t)(t,h)(h,e)(e, )( ,s)', '(d,e)(e,r)(r,s)(s, )( ,a)(a, )( ,c)(c,e)(e,n)(n,s)(s,u)', '(i,c)(c, )( ,d)(d,o)(o,m)(m,i)(i,n)(n,a)(a,t)(t,i)(i,o)', '(e, )( ,o)(o,f)(f, )( ,g)(g,r)(r,e)(e,a)(a,t)(t, )( ,i)', '(r,d)(d,s)(s, )( ,a)(a,n)(n,d)(d, )( ,h)(h,o)(o,l)(l,d)', '(u,k)(k, )( ,i)(i,n)(n,l)(l,a)(a,n)(n,d)(d, )( ,h)(h,o)', '(b,l)(l,e)(e, )( ,f)(f,o)(o,r)(r, )( ,t)(t,h)(h,e)(e, )', '(e,i)(i,g)(g,h)(h,t)(t, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )', '(n,g)(g, )( ,t)(t,h)(h,e)(e, )( ,s)(s,u)(u,f)(f,f)(f,i)', '(i,o)(o,n)(n,s)(s, )( ,d)(d,o)(o,e)(e,s)(s, )( ,n)(n,o)', '(r,e)(e,s)(s,e)(e,r)(r,v)(v,a)(a,t)(t,i)(i,o)(o,n)(n, )', '(r, )( ,e)(e,x)(x,a)(a,m)(m,p)(p,l)(l,e)(e, )( ,m)(m,o)', '(h,t)(t, )( ,h)(h,e)(e, )( ,s)(s,e)(e,t)(t,t)(t,l)(l,e)', '( ,m)(m,u)(u,s)(s,c)(c,l)(l,e)(e, )( ,f)(f,i)(i,b)(b,r)', '(e,s)(s,o)(o,u)(u,r)(r,c)(c,e)(e,s)(s, )( ,a)(a,r)(r,e)', '(i,e)(e,v)(v,e)(e, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e, )', '(n, )( ,s)(s,e)(e,p)(p,t)(t,e)(e,m)(m,b)(b,e)(e,r)(r, )', '(t, )( ,a)(a,n)(n,d)(d, )( ,e)(e,s)(s,p)(p,e)(e,c)(c,i)', '(e,n)(n,d)(d, )( ,h)(h,u)(u,n)(n,d)(d,r)(r,e)(e,d)(d,s)', '(r,a)(a,l)(l, )( ,s)(s,e)(e,n)(n,s)(s,e)(e, )( ,c)(c,a)', '(r,e)(e,q)(q,u)(u,i)(i,r)(r,e)(e,m)(m,e)(e,n)(n,t)(t,s)', '(j,a)(a,p)(p,a)(a,n)(n, )( ,w)(w,h)(h,e)(e,r)(r,e)(e, )', '(t,e)(e,r)(r,v)(v,i)(i,e)(e,w)(w,s)(s, )( ,f)(f,o)(o,c)', '(r,y)(y, )( ,w)(w,a)(a,r)(r, )( ,b)(b,r)(r,i)(i,t)(t,i)', '(d, )( ,a)(a,i)(i,r)(r,c)(c,r)(r,a)(a,f)(f,t)(t, )( ,c)', '(t,h)(h,e)(e, )( ,p)(p,h)(h,y)(y,s)(s,i)(i,c)(c,a)(a,l)', '(e,r)(r, )( ,t)(t,h)(h,e)(e, )( ,r)(r,e)(e,l)(l,a)(a,t)', '(h,e)(e,m)(m,a)(a,t)(t,i)(i,c)(c,a)(a,l)(l, )( ,m)(m,e)', '( ,o)(o,f)(f, )( ,e)(e,d)(d,e)(e,n)(n, )( ,i)(i,s)(s, )', '(t,i)(i,t)(t,u)(u,t)(t,i)(i,o)(o,n)(n,a)(a,l)(l, )( ,a)', '(i,n)(n,e)(e,n)(n,t)(t, )( ,a)(a,n)(n,d)(d, )( ,t)(t,h)', '(u,d)(d,e)(e, )( ,a)(a, )( ,o)(o, )( ,a)(a,n)(n,d)(d, )', '(n,g)(g, )( ,t)(t,h)(h,e)(e, )( ,l)(l,i)(i,n)(n,e)(e,a)', '(n,g)(g,u)(u,a)(a,g)(g,e)(e,s)(s, )( ,w)(w,e)(e,r)(r,e)', '(b,e)(e,t)(t,t)(t,e)(e,r)(r, )( ,c)(c,o)(o,n)(n,t)(t,r)', '( ,n)(n,i)(i,n)(n,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )', '( ,p)(p,o)(o,r)(r,t)(t, )( ,o)(o,f)(f, )( ,s)(s,o)(o,l)', '(i,c)(c,k)(k, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)', '(t,h)(h,e)(e,r)(r, )( ,p)(p,r)(r,a)(a,i)(i,s)(s,e)(e, )', '(o,l)(l,i)(i,c)(c,y)(y, )( ,a)(a,f)(f,t)(t,e)(e,r)(r, )', '(c,h)(h, )( ,i)(i,n)(n,v)(v,o)(o,l)(l,v)(v,e)(e,s)(s, )', '(r,d)(d,e)(e,r)(r,e)(e,d)(d, )( ,t)(t,o)(o, )( ,m)(m,o)', '(k,u)(u,n)(n,i)(i, )( ,t)(t,h)(h,e)(e, )( ,f)(f,i)(i,r)', '(a,l)(l, )( ,e)(e,l)(l,e)(e,c)(c,t)(t,i)(i,o)(o,n)(n,s)', '(g,n)(n,i)(i,f)(f,i)(i,c)(c,a)(a,n)(n,t)(t, )( ,i)(i,n)', '(o, )( ,t)(t,h)(h,e)(e, )( ,a)(a,c)(c,c)(c,o)(o,m)(m,p)', '(c,l)(l,o)(o,p)(p,a)(a,e)(e,d)(d,i)(i,a)(a, )( ,o)(o,f)', '(a,n)(n,d)(d, )( ,a)(a,i)(i,r)(r, )( ,d)(d,e)(e,f)(f,e)', '(g, )( ,e)(e,x)(x,a)(a,m)(m, )( ,a)(a,d)(d,m)(m,i)(i,n)', '(a,y)(y, )( ,m)(m,a)(a,y)(y, )( ,t)(t,w)(w,o)(o, )( ,s)', '(o,d)(d,e)(e,r)(r,n)(n, )( ,d)(d,a)(a,y)(y, )( ,m)(m,o)', '(a,l)(l, )( ,b)(b,u)(u,d)(d,d)(d,h)(h,i)(i,s)(s,m)(m, )', '( ,h)(h,a)(a,v)(v,e)(e, )( ,m)(m,a)(a,d)(d,e)(e, )( ,s)']\n",
      "['( ,a)(a,n)']\n",
      "['(a,n)(n,a)']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "bigram_size = vocabulary_size * vocabulary_size\n",
    "\n",
    "class BigramBatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, bigram_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      first_char = self._text[self._cursor[b]]\n",
    "      if self._cursor[b] + 1 == self._text_size:\n",
    "        second_char = ' '\n",
    "      else:\n",
    "        second_char = self._text[self._cursor[b] + 1]\n",
    "      batch[b, char2id(first_char) * vocabulary_size + char2id(second_char)] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def bigram_characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return ['({0},{1})'.format(id2char(c//vocabulary_size), id2char(c % vocabulary_size))\n",
    "          for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def bigram_first_characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c//vocabulary_size)\n",
    "          for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def bigram_batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, bigram_characters(b))]\n",
    "  return s\n",
    "\n",
    "bigram_train_batches = BigramBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "bigram_valid_batches = BigramBatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "# output each bigram instead of a single char\n",
    "print(bigram_batches2string(bigram_train_batches.next()))\n",
    "print(bigram_batches2string(bigram_train_batches.next()))\n",
    "print(bigram_batches2string(bigram_valid_batches.next()))\n",
    "print(bigram_batches2string(bigram_valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # big matrix\n",
    "  ifcox = tf.Variable(tf.truncated_normal([bigram_size, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifcob = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, bigram_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([bigram_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    all_gates = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "    input_gate = tf.sigmoid(all_gates[:, 0:num_nodes])\n",
    "    forget_gate = tf.sigmoid(all_gates[:, num_nodes:2*num_nodes])\n",
    "    update = all_gates[:, 2*num_nodes:3*num_nodes]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(all_gates[:, 3*num_nodes:])\n",
    "        \n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,bigram_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs,0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.concat(train_labels,0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, bigram_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bigram_sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, bigram_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def bigram_random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, bigram_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 6.591044 learning rate: 10.000000\n",
      "Minibatch perplexity: 728.54\n",
      "================================================================================\n",
      "bigrams: (z,s)(h,l)(y,w)(c,b)( ,o)(b,b)(x, )(l,e)(h,r)(o,m)(c,l)(f,e)(g,c)(o,q)(b,l)( ,f)(f,d)(p,z)(u,i)(a,y)(b,d)(n,v)(r,p)(l,b)(f,u)(x,x)(i,u)(p,x)(k,g)(s,b)(j,l)(c,l)(w,u)(t,o)(e,b)(t,a)(r,m)(u,c)(s,i)(h,i)(n,i)(x,d)(l,r)(o,e)(h,v)(f,r)( ,c)(f,x)(k,s)(a,g)(a,t)(i,r)(c,s)(z,p)(j,p)( ,w)(n,u)(q, )(f,c)(d,d)(f,j)(g,a)(x,a)(e,g)(c,a)(y,r)(q,i)(n,b)(j, )(e,v)(c,f)(h,g)(q,o)(m,u)(f,f)(n,l)(a,f)(v, )(x,r)(p,e)\n",
      "chars: zhyc bxlhocfgob fpuabnrlfxipksjcwtetrushnxlohf fkaaiczj nqfdfgxecyqnjechqmfnavxp\n",
      "bigrams: (s,z)(c,w)(l,x)(f,u)(h,o)(b,s)(v,y)(k,m)(r,x)(k,q)(h,g)(h,b)(n,n)(i,n)(y,p)(r, )(r,l)(a,g)(f,s)(e,e)(j,j)(b,h)(b,v)(m,y)(i,l)(a,l)(w,k)(a, )( , )(j,c)(s, )(t,d)(q,x)(o,m)(b,m)(c,g)(u,o)(d,m)(t,r)(g,d)(e,k)(w,u)(b, )(c,t)(c,z)(i,g)( ,l)(s,l)(c,k)(q,e)(c,i)(h,n)(p,n)(t,x)(q,k)(g,s)(w,s)(q, )(k, )(d,t)(y,m)(v,i)(x,h)(r,i)(h,x)(v,e)(e,q)(j,m)(d,c)(e,n)(n,d)(s,x)(t,k)(w,y)(m,j)(v,k)(g,r)(m, )(z,c)(a,t)\n",
      "chars: sclfhbvkrkhhniyrrafejbbmiawa jstqobcudtgewbcci scqchptqgwqkdyvxrhvejdenstwmvgmza\n",
      "bigrams: (y,h)(e,u)(k,i)(h,c)(n,i)(h,q)(w,m)(w,l)(q,l)(s,u)(j,d)(o,p)(l,c)(r,y)(u,q)(y,s)(r,s)( ,i)(q,m)(j,s)(j,w)(c, )(d, )(x,q)(f,v)(i,s)(d,k)(d,u)(b,y)(m,b)(k,y)(e,e)(c,e)( ,t)(i,m)(n,j)(h,j)(r,h)(k,v)(t,f)( ,v)(n,j)(v,t)(a,y)(q,m)(x,h)(u,z)(l,l)(i,o)(u,q)(v,f)(g,g)(l,o)(z,s)(z,o)(c,n)(e,e)(z,i)(r,u)(h,q)(c,f)(w,w)(k,d)(p,e)(m,m)(b,j)(j,l)(f,d)(w,h)(t,i)(r,s)(g,t)(h,o)(g,f)(i,t)(y,a)(k,a)(j, )(t,s)(z,c)\n",
      "chars: yekhnhwwqsjolruyr qjjcdxfiddbmkec inhrkt nvaqxuliuvglzzcezrhcwkpmbjfwtrghgiykjtz\n",
      "bigrams: (f,r)(r,h)(t,g)(v,e)(g, )(n,b)(j,r)(a, )(o,u)(f,j)(f,n)(h,f)(k, )(i,m)(q,l)(g,f)(u,b)(l,s)(t,e)(w,o)(b,i)(v,o)(v,e)(g,c)(j,k)(f,b)(c,b)(r,c)(l,n)(r,f)(x,b)(e,v)(z,u)(s,c)(e,c)(l,b)(a,f)(z,k)(x,g)(w,n)(h,o)(b,t)(r,d)(p,b)(q,q)(a,z)(d,b)(v,c)(t,j)(l,f)(d,t)(t,k)(w,o)(u,j)(e,g)(f,b)(f,j)(v,e)(y,a)( ,o)(i,c)(m,d)(x,y)(v,b)(i,e)(p,e)(i,t)(r,m)( ,t)( ,x)(k,l)(h,z)(l,n)(i,u)(n,e)(y,y)(t,z)(v,t)(u,i)(m, )\n",
      "chars: frtvgnjaoffhkiqgultwbvvgjfcrlrxezselazxwhbrpqadvtldtwueffvy imxvipir  khlinytvum\n",
      "bigrams: (d,a)(w,h)(e,y)(e,y)(z,z)(z,q)(d,e)(d,a)(e,v)(a,m)(w,w)(e,h)(t,p)(w,v)(l,e)(u,a)(f,i)(a,z)(e,v)(e,o)(c,v)(x,c)(t,o)(a,z)(s,a)(t,k)(c,v)(e,m)(r,d)(u,i)(a,v)(t,i)(d,w)(r,p)(z,d)(s,a)(h,z)(j, )(c,v)(n,p)(k,c)(x,b)(t,f)(c,j)(z,y)(r, )(j,v)(e,v)(y,s)(s,w)(j,w)(s,v)(z,w)(q,b)(o,j)(c,a)(a,n)(b,j)(w,k)(n,t)(d,n)(p,o)(l,k)(j,d)(p,q)(e,a)(s,k)(l,c)(n,l)(j,x)(t,c)(e,m)(h,f)(w,a)(n,q)(f,w)(s,w)(c, )(h,k)(e,w)\n",
      "chars: dweezzddeawetwlufaeecxtastceruatdrzshjcnkxtczrjeysjszqocabwndpljpeslnjtehwnfsche\n",
      "================================================================================\n",
      "Validation set perplexity: 674.81\n",
      "Average loss at step 100: 5.247770 learning rate: 10.000000\n",
      "Minibatch perplexity: 103.73\n",
      "Validation set perplexity: 106.34\n",
      "Average loss at step 200: 3.653049 learning rate: 10.000000\n",
      "Minibatch perplexity: 16.90\n",
      "Validation set perplexity: 19.62\n",
      "Average loss at step 300: 2.559414 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.82\n",
      "Validation set perplexity: 11.52\n",
      "Average loss at step 400: 2.204962 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.60\n",
      "Validation set perplexity: 8.68\n",
      "Average loss at step 500: 2.077759 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.28\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 600: 1.976345 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 700: 1.880638 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.04\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 800: 1.866543 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.78\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 900: 1.857303 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 1000: 1.824528 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "================================================================================\n",
      "bigrams: (q,t)(o,g)(t,k)(i,l)(l,y)(y, )( ,a)(a, )( ,s)(s,u)(u,m)(m, )( ,n)(n,i)(i,n)(n,e)(e,n)(n,t)(t,s)(s,k)(h,n)(n,i)(i,u)(a, )( ,e)(e,g)(g,u)(u,e)(e,s)(s,t)(t,a)(a,t)(t,i)(i,e)(e,d)(d, )( ,a)(a,s)(s, )( ,a)(a,c)(c,t)(t,e)(e,d)(d, )( ,i)(i,n)(n, )( ,o)(o,f)(f, )( ,c)(c,e)(e,n)(n,d)(p, )( ,p)(p,r)(r,o)(o,b)(b,e)(b,e)(e,n)(n,e)(e,n)(n,t)(t,i)(i,m)(m, )( ,a)(a,r)(r,e)(e, )( ,c)(c,h)(h,a)(a,c)(c,k)(k, )( ,t)\n",
      "chars: qotily a sum ninentshnia eguestatied as acted in of cenp probbenentim are chack \n",
      "bigrams: (s,h)(h, )( ,m)(m,i)(i,c)(c,h)(h,o)(o, )( ,l)(l,o)(o,u)(u,g)(g,h)(h,t)(t, )( ,f)(f,a)(a,m)(m,a)(a,c)(c,e)(e,s)(s, )( ,a)(a,s)(s, )( ,t)(t,h)(h,a)(a,t)(t, )( ,e)(e,x)(x,i)(i,f)(f, )( ,a)(a, )( ,p)(p,r)(r,e)(e,g)(g,i)(i,m)(m,e)(e,s)(s, )( ,a)(a,r)(r,e)(e, )( ,g)(g,a)(a,m)(m,e)(e,s)(s, )( ,t)(t,h)(h,e)(e, )( ,f)(w,a)(a,s)(s,t)(t,h)(h,o)(o,w)(w,e)(e,r)(r, )( ,a)(a,s)(s,e)(e,n)(n, )( ,t)(t,o)(o, )( ,t)\n",
      "chars: sh micho lought famaces as that exif a pregimes are games the wasthower asen to \n",
      "bigrams: (w,e)(e, )( ,r)(r,e)(e,t)(t,o)(o,v)(v,e)(e,r)(r, )( ,b)(b,y)(y, )( ,p)(p,r)(r,o)(o,m)(m,i)(i,t)(t,r)(r,a)(a,n)(n,i)(i,a)(a,l)(l, )( ,l)(l,i)(i,s)(s,t)(t, )( ,p)(p,r)(r,a)(a,i)(i,l)(l, )( ,a)(a,n)(n,d)(d, )( ,s)(s,y)(y,s)(s,t)(t,e)(e,n)(n,c)(c,e)(e,d)(d, )( ,a)(a,d)(d,e)(e,d)(d, )( ,c)(c,o)(o,u)(u,n)(n,d)(d,e)(e,s)(s, )( ,h)(h, )( ,n)(n,o)(o,t)(t,s)(s,i)(i,c)(c, )( ,r)(r,e)(e,g)(g,r)(r,a)(a,n)(n,c)\n",
      "chars: we retover by promitranial list prail and systenced aded coundes h notsic regran\n",
      "bigrams: (i,q)(t,t)(t,i)(i,a)(a, )( ,v)(v,i)(i,g)(g,h)(h,t)(t, )( ,p)(p,a)(a,r)(r,g)(g,r)(r,a)(a,g)(g,i)(i,t)(t,y)(y, )( ,i)(i,n)(n, )( ,o)(o,t)(t,h)(h,i)(i,s)(s,m)(m, )( ,a)(a,t)(t, )( ,p)(p,r)(r,o)(o,p)(p,e)(e,d)(d, )( ,t)(t,u)(u,r)(r,n)(n, )( ,c)(c,u)(u,s)(s,t)(t, )( ,r)(r,e)(e,v)(f,f)(f,i)(i,e)(e,n)(n,d)(d,e)(e,d)(d, )( ,a)(a,n)(n,d)(d, )( ,u)(u,n)(n,i)(i,k)(b,r)(r,e)(e,f)(f,t)(t,e)(e,d)(d, )( ,i)(i,n)\n",
      "chars: ittia vight pargragity in othism at proped turn cust reffiended and unibrefted i\n",
      "bigrams: (n,c)(c,h)(c,h)(h,o)(o,l)(l, )( ,b)(b, )( ,p)(p,e)(e,c)(c,a)(a,r)(r,d)(d,s)(s, )( ,r)(r,e)(e,a)(a,c)(c,h)(h,e)(e,d)(d, )( ,w)(w,a)(a,s)(s, )( ,a)(a,s)(s, )( ,p)(p,e)(e,r)(r,t)(t,i)(i,o)(o,n)(n, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,r)(r,e)(e,v)(v,i)(i,s)(s, )( ,a)(a, )( ,f)(f,o)(o,u)(u,r)(r, )( ,a)(a, )( ,h)(h,o)(o,v)(v,e)(e,d)(d, )( ,a)(a,n)(n,d)(d, )( ,i)(i,n)(n,t)(t,e)(e,m)(m,p)(p,i)(i,n)(n,e)\n",
      "chars: ncchol b pecards reached was as pertion of the revis a four a hoved and intempin\n",
      "================================================================================\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 1100: 1.761962 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 1200: 1.745997 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 1300: 1.721146 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 1400: 1.738867 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1500: 1.712812 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1600: 1.716081 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.69\n",
      "Average loss at step 1700: 1.685216 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 1800: 1.640830 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1900: 1.646574 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 2000: 1.661447 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "================================================================================\n",
      "bigrams: (e,e)(e,o)(o,u)(u,s)(s, )( ,p)(p,r)(r,e)(e,s)(s,i)(r,e)(e,e)(e, )( ,a)(a,f)(f,t)(t,e)(e,r)(r, )( ,h)(h,i)(i,r)(r,m)(m,s)(s, )( ,n)(n,f)(f,o)(o,r)(r,t)(t,a)(a,l)(l,e)(e,r)(r, )( ,k)(k,l)(l,e)(e, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e, )( ,p)( ,r)(r,a)(a,i)(i,n)(n,l)(l,y)(y, )( ,b)(b,e)(e,i)(i,n)(n,g)(g, )( ,a)(a,d)(d,v)(v,e)(e,r)(r,c)(c,e)(e, )( ,f)(f,a)(a,i)(i,r)(r,s)(s,o)(o,n)(n,e)(e, )( ,h)(h,a)(a,t)\n",
      "chars: eeous presree after hirms nfortaler kle in the  rainly being adverce fairsone ha\n",
      "bigrams: (p,e)(e,d)(d, )( ,w)(w,e)(e,r)(r,e)(e, )( ,h)(h,e)(e,a)(a,d)(d, )( ,p)(p,a)(a,d)(l,k)(a,y)(y, )( ,k)(k,i)(i,n)(n,g)(g, )( ,t)(t,h)(h,e)(e, )( ,b)(b,o)(o,t)(t,h)(h, )( ,t)(t,h)(h,e)(e, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,e)(e,a)(a,r)(r,g)(u,k)(m,u)(h,y)(y, )( ,c)(c,o)(o,m)(m,e)(e,i)(i,n)(n,e)(e, )( ,a)(a, )( ,c)(c,o)(o,u)(u,r)(r,t)(t,s)(s, )( ,o)(o,f)(f, )\n",
      "chars: ped were head palay king the both the one nine eight earumhy comeine a courts of\n",
      "bigrams: (c,p)(a,d)(d, )( ,o)(o,f)(f, )( ,m)(m,e)(e,a)(a,n)(n, )( ,p)(p,o)(o,p)(p,a)(a,h)(h,e)(e,d)(d, )( ,t)(t,o)(o, )( ,d)(d,e)(e,q)(q,u)(u,a)(a,l)(l,g)(g,e)(e,l)(l,a)(a, )( ,e)(e,s)(s,c)(c,h)(h,o)(o,m)(m, )( ,i)(i,n)(n,f)(f,i)(i,n)(n,e)(e, )( ,p)(p,l)(l,a)(a,k)(k,e)(e,b)(b,a)(a,r)(r,l)(l, )( ,t)(t,o)(o, )( ,c)(c,l)(l,a)(a,r)(r,g)(g,e)(e,d)(d, )( ,f)(f,r)(r,e)(e,e)(e,m)(m,e)(e,r)(r, )( ,i)(i,n)(n, )( ,m)\n",
      "chars: cad of mean popahed to dequalgela eschom infine plakebarl to clarged freemer in \n",
      "bigrams: (m,k)(o,z)(z, )( ,i)(i,r)(r,a)(a, )( ,n)(n,o)(o,u)(u,n)(n,t)(t, )( ,a)(a,n)(n,d)(d, )( ,m)(m,a)(a,r)(r,i)(i,l)(l, )( ,r)(r,e)(e,p)(j,e)(e,r)(r,s)(s,o)(o,n)(n, )( ,t)(t,v)(h,a)(a,n)(n,s)(s,k)(k,i)(i,n)(n,g)(g, )( ,t)(t,h)(c, )( ,a)(a, )( ,d)(d,i)(i,s)(s,t)(t,a)(a,n)(n,t)(t,i)(i,o)(o,n)(n, )( ,w)(w,a)(a,s)(s, )( ,h)(h,e)(e,r)(r,o)(o,u)(u,n)(n,d)(d,r)(r,y)(y,s)(s,h)(h,a)(a,n)(n,c)(c,e)(e, )( ,p)(p,r)\n",
      "chars: moz ira nount and maril rejerson thansking tc a distantion was heroundryshance p\n",
      "bigrams: (k,l)(l,y)(y, )( ,a)(a,n)(n,d)(d, )( ,e)(e,x)(x,c)(c,h)(h,e)(e,m)(m, )( ,b)(b,e)(e,r)(r,m)(m,u)(j,e)(e,h)(h, )( ,c)(c,o)(o,n)(n,v)(v,e)(e,r)(r,r)(r,e)(e,d)(d, )( ,t)(t,r)(r,a)(a,n)(n,n)(n,s)(s,t)(t, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,f)(f,o)(o,u)(u,r)(r, )( ,y)(y,o)(o,s)(s,t)(t, )( ,b)(b,e)(e,l)(l,o)(o,r)(r,s)(s, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,h)(h,i)(i,d)(d,e)(e,a)(a, )( ,p)\n",
      "chars: kly and exchem bermjeh converred trannst one nine four yost belors of the hidea \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2100: 1.650484 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 2200: 1.620249 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2300: 1.604523 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2400: 1.639684 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2500: 1.635950 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2600: 1.615664 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2700: 1.616245 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 2800: 1.614063 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 2900: 1.599614 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 3000: 1.597163 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      "bigrams: (c,a)(a,p)(p,e)(e,n)(n,o)(o,m)(m,y)(y, )( ,e)(e,g)(g,a)(a,i)(i,n)(n,s)(s,t)(t,e)(e,g)(g,o)(o,b)(b,i)(i,t)(t,e)(e,t)(t, )( ,o)(o,w)(w,n)(n, )( ,r)(r,o)(o, )( ,u)(u,s)(s,e)(e,d)(d,u)(u,c)(c,t)(t,i)(i,o)(o,n)(n, )( ,h)(h,e)(e,a)(a,v)(v,i)(i,a)(a,n)(n, )( ,f)(f,o)(o,r)(r, )( ,h)(h,i)(i,m)(m, )( ,s)(s,y)(y,s)(s, )( ,o)(o,f)(f, )( ,m)(m,a)(a,x)(v,i)(i,f)(f,i)(i,c)(c,a)(a,l)(l,l)(l,y)(y, )( ,t)(t,a)(a,b)\n",
      "chars: capenomy egainstegobitet own ro useduction heavian for him sys of mavifically ta\n",
      "bigrams: (a,x)(l,g)(g,o)(o, )( ,t)(t,r)(r,a)(a,n)(n,s)(s,l)(l,a)(a,t)(t,i)(i,v)(v,e)(e,s)(s, )( ,t)(t,h)(h,e)(e, )( ,t)(t,h)(h,e)(e,i)(i,r)(r, )( ,m)(m,e)(e,a)(a,n)(n, )( ,s)(s,e)(e,a)(a,l)(l, )( ,b)(b,a)(a,n)(n,i)(i,s)(s,h)(h,t)(t, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,l)(l,i)(i,k)(k,i)(i,n)(n,g)(g, )( ,a)(a,n)(n,d)(d, )( ,t)(t,h)(h,e)(e, )( ,c)(c,h)(h,i)(i,n)(n,e)(e,y)(y, )( ,t)(t,h)(h,e)(e,o)(o,r)(r,i)\n",
      "chars: algo translatives the their mean seal banisht of the liking and the chiney theor\n",
      "bigrams: (b,u)(u,s)(s,e)(e, )( ,n)(n,i)(i,s)(s,t)(t,i)(i,c)(c,i)(i,t)(t,y)(y, )( ,t)(t,e)(e,n)(n,s)(s,i)(i,t)(t,y)(y, )( ,p)(p,r)(r,e)(e,v)(v,e)(e,n)(n,t)(t, )( ,q)(q,u)(u,i)(i,x)(y,i)(i,n)(n,o)(o,r)(r, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,a)(a,p)(p,p)(p,e)(e,a)(a,r)(r,e)(e,d)(d, )( ,f)(f,o)(o,u)(u,r)(r, )( ,y)(y,e)(e,a)(a,r)(r, )( ,c)(c,k)(d,u)(u,r)(r,y)(z, )( ,u)(u,n)(n,s)(s,t)(t,i)(i,t)(t,i)(i,o)(o,n)\n",
      "chars: buse nisticity tensity prevent quiyinor of the appeared four year cdurz unstitio\n",
      "bigrams: (k,s)(s,a)(a,n)(n, )( ,h)(h,e)(e,y)(y, )(y, )( ,a)(a,r)(r,g)(g,e)(e,n)(n,t)(t, )( ,p)(p,r)(r,e)(e,d)(d,i)(i,t)(t,i)(i,o)(o,n)(n, )( ,s)(s,t)(t,o)(o,r)(r,i)(i,n)(n,g)(g, )( ,m)(m,a)(a,l)(l,u)(u,e)(e,u)(u,m)(m, )(s,h)(h,i)(i, )( ,s)(s,c)(c,o)(o,u)(u,r)(r,n)(n,i)(i,t)(t,y)(y, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,f)(f,e)(e,w)(w,b)(l,o)(o,n)(n,a)(a, )( ,u)(u,s)(s,i)(i,n)(n, )( ,o)(o,r)(r, )( ,h)(h,a)\n",
      "chars: ksan heyy argent predition storing malueumshi scournity of the fewlona usin or h\n",
      "bigrams: (r,a)(a,p)(p,r)(r,e)(e,s)(s,o)(o,m)(m,e)(e, )( ,i)(i,t)(t,s)(s, )( ,n)(n, )( ,n)(n,u)(u,n)(n,c)(c,h)(h,a)(a,l)(l,l)(l,e)(e,d)(d, )( ,a)(a,s)(s, )( ,f)(f,o)(o,r)(r, )( ,h)(h,a)(a,n)(n,d)(d, )( ,s)(s,y)(y,m)(m,e)(e,r)(r,i)(i,s)(s,t)(t, )( ,c)(c,o)(o,p)(p,i)(i,l)(l, )( ,c)(c,o)(o,m)(m,m)(m,o)(o,s)(s, )( ,w)(w,a)(a,y)(y, )( ,o)(o,t)(t,h)(h,e)(e,r)(r, )( ,o)(o,r)(r, )( ,t)(t,h)(h,e)(e, )( ,n)(n, )( ,w)\n",
      "chars: rapresome its n nunchalled as for hand symerist copil commos way other or the n \n",
      "================================================================================\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3100: 1.583277 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3200: 1.602501 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3300: 1.616658 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 3400: 1.613546 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3500: 1.600573 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3600: 1.608331 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3700: 1.597144 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3800: 1.587742 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3900: 1.573720 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4000: 1.601655 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "bigrams: (x,d)(s,o)(o,w)(w,s)(s, )( ,o)(o,f)(f, )( ,g)(g,o)(o,r)(r,a)(a, )( ,t)(t,o)(o, )( ,b)(b,e)(e, )( ,a)(a,s)(s,p)(p,r)(r,e)(e,s)(s,e)(e,r)(r,s)(s,t)(t,y)(y, )( ,a)(a,s)(s, )( ,r)(r,e)(e,d)(d,e)(e,r)(r,s)(s,m)(m,a)(a,r)(r, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,w)(w,a)(a,y)(y,a)(a,c)(c,k)(k,s)(s, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,d)(d,i)(i,f)(f,f)(f,i)(i,c)(c,i)(i,e)(e,n)(n,c)(c,i)(i,s)(s,h)(h,t)\n",
      "chars: xsows of gora to be aspresersty as redersmar of the wayacks of the difficiencish\n",
      "bigrams: (j,j)(d,f)(f,a)(a,n)(n,t)(t, )( ,a)(a,a)(j,w)(c,u)(m,i)(i,m)(m,e)(e,d)(d, )( ,m)(m,a)(a,y)(y, )( ,f)(f,o)(o,r)(r, )( ,g)(g,e)(e,n)(n,e)(e,w)(a,l)(l,s)(s, )( ,a)(a, )( ,h)(h,e)(e,r)(r, )( ,s)(s,o)(o,c)(c,i)(i,a)(a,l)(l, )( ,p)(p,a)(a,c)(c,t)(p,m)(w,a)(a,t)(t,e)(e,d)(d, )( ,a)(a, )( ,f)(f,e)(e,s)(s,t)(t,a)(a,n)(n,t)(t, )( ,f)(f,u)(u,l)(l,t)(t,i)(i,o)(o,n)(n, )( ,l)(l,o)(o,r)(r,m)(m,a)(a,n)(n, )( ,l)\n",
      "chars: jdfant ajcmimed may for geneals a her social pacpwated a festant fultion lorman \n",
      "bigrams: (f,j)(d,r)(r,o)(o,r)(r,s)(s, )( ,t)(t,o)(o,m)(m, )( ,l)(l,i)(i,s)(s,t)(t, )( ,o)(o,v)(v,a)(a,i)(i,d)(d, )( ,t)(t,h)(h,e)(e, )( ,p)(p,u)(u,b)(b,l)(l,i)(i,c)(c,e)(e,d)(d, )( ,a)(a,s)(s, )( ,i)(i,l)(l,l)(l,a)(a,n)(n,n)(n,e)(e,d)(d, )( ,b)(b,y)(y, )( ,e)(e,a)(a,r)(r,t)(t,y)(y, )( ,a)(a,s)(s, )( ,c)(c,i)(i,t)(t,o)(o,r)(r, )( ,w)(w,o)(o,r)(r,k)(k,s)(s,y)(y,r)(r,i)(i,d)(d, )( ,t)(t,o)(o, )( ,g)(g,e)(e,n)\n",
      "chars: fdrors tom list ovaid the publiced as illanned by earty as citor worksyrid to ge\n",
      "bigrams: (q,p)(a,e)(e,l)(l,f)(f, )( ,m)(m,a)(a,n)(n,n)(n,a)(a,m)(m, )( ,y)(y,e)(e,d)(d, )( ,i)(i,n)(n, )( ,c)(c,h)(h,i)(i,l)(s,f)(d,r)(r,e)(e,n)(n,s)(s,t)(t, )( ,c)(c,o)(o,u)(u,n)(n,t)(t, )( ,f)(f,a)(a,m)(m,a)(a,l)(l,i)(i,s)(s,h)(h,e)(e,s)(s, )( ,o)(o,n)(n,e)(e, )( ,t)(t,h)(h,r)(r,e)(e,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,t)(t,h)(h,r)(r,e)(e,e)(e, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e, )( ,e)(e,n)(n,d)(d, )( ,o)\n",
      "chars: qaelf mannam yed in chisdrenst count famalishes one three nine three in the end \n",
      "bigrams: (z,a)(c,s)(s,e)(e, )( ,s)(s,e)(e,e)(e,p)(p,h)(h,a)(a,b)(b,i)(i,m)(m,i)(i,c)(c, )( ,b)(b,a)(a,r)(r,n)(n, )( ,i)(i,s)(s, )( ,h)(h,e)(e,a)(a,k)(k, )( ,t)(t,r)(r,a)(a,b)(b,a)(a,n)(n,d)(d, )( ,s)(s,h)(h,o)(o,d)(d,r)(r,e)(e,s)(s, )( ,u)(u,s)(s,e)(e,d)(d, )( ,i)(i,n)(n, )( ,s)(s,t)(t,r)(r,u)(u,c)(c,t)(t, )( ,o)(o,n)(n, )( ,h)(h,i)(i,s)(s, )( ,h)(h,a)(a,r)(r,d)(d,e)(e,d)(d,i)(i,a)(a,n)(n, )( ,l)(l,o)(o,n)\n",
      "chars: zcse seephabimic barn is heak traband shodres used in struct on his hardedian lo\n",
      "================================================================================\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4100: 1.567006 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4200: 1.568241 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4300: 1.548567 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4400: 1.543826 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4500: 1.549782 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4600: 1.575417 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4700: 1.557558 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4800: 1.568779 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4900: 1.554103 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5000: 1.537794 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "================================================================================\n",
      "bigrams: (m,y)(y, )( ,a)(a,s)(s, )( ,t)(t,h)(h,e)(e, )( ,f)(f,l)(l,e)(e,g)(g,o)(o,i)(i,d)(d,s)(s, )( ,m)(m,u)(u,c)(c,k)(k,e)(e,y)(y, )( ,a)(a,p)(p,p)(p,e)(e,a)(a,r)(r,s)(s, )( ,r)(r,a)(a,l)(l,e)(e,m)(m,o)(o,n)(n, )( ,a)(a,p)(p,p)(p,r)(r,o)(o,x)(h,c)(c,y)(y, )( ,t)(t,h)(h,e)(e, )( ,p)(p,r)(r,o)(o,t)(t,i)(i,c)(c,t)(t,i)(i,n)(n,g)(g, )( ,s)(s,t)(t,a)(a,s)(o,w)(f,r)(r,a)(a,g)(g,e)(e, )( ,c)(c,o)(o,n)(n,t)(t,a)\n",
      "chars: my as the flegoids muckey appears ralemon approhcy the proticting staofrage cont\n",
      "bigrams: (c,h)(h,e)(e,s)(s, )( ,u)(u,s)(s,e)(e,s)(s, )( ,i)(i,n)(n, )( ,y)(y,o)(o,n)(n,g)(g, )( ,a)(a,m)(m,e)(e,r)(r,i)(i,c)(c,a)(a,n)(n, )( ,r)(r,o)(o,w)(w, )( ,t)(t,h)(h,e)(e, )( ,t)(t,r)(r,a)(a,i)(i,n)(n,i)(i,n)(n,g)(g, )( ,i)(i,t)(t, )( ,o)(o,n)(n, )( ,m)(m,a)(a,r)(r,i)(i,t)(t,e)(e,r)(r, )( ,i)(i,n)(n,d)(d, )( ,t)(t,h)(h,e)(e, )( ,f)(f,i)(i,x)(v,e)(e,n)(n,i)(i,e)(e,s)(s, )( ,m)(m,e)(e,t)(t,a)(a,n)(n,a)\n",
      "chars: ches uses in yong american row the training it on mariter ind the fivenies metan\n",
      "bigrams: (x,l)(l,i)(i,p)(p,l)(l,i)(i, )( ,n)(n,o)(o,t)(t,o)(o,l)(l, )( ,o)(o,n)(n, )( ,f)(f,o)(o,u)(u,r)(r,c)(c,e)(e, )( ,t)(t,h)(h,e)(e, )( ,o)(o,t)(t,h)(h,e)(e,r)(r, )( ,e)(e,f)(f,f)(f,e)(e,c)(c,t)(t,i)(i,o)(o,n)(n,s)(s, )( ,c)(c,o)(o,n)(n,s)(s,i)(i,t)(t,i)(i,b)(b,l)(l,e)(e, )( ,i)(i,n)(n,t)(t,e)(e,t)(t,i)(i,c)(c,l)(l,o)(o,o)(o,d)(d, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e, )( ,s)(s,c)(c,h)(h,o)(o,o)(o,l)(l,p)\n",
      "chars: xlipli notol on fource the other effections consitible inteticlood in the school\n",
      "bigrams: (f,y)(y,n)(n,t)(t,o)(o,r)(r,i)(i,t)(t,y)(y, )( ,t)(t,o)(o, )( ,e)(e,x)(x,p)(p,e)(e,r)(r,p)(p,t)(t,i)(i,o)(o,n)(n,s)(s, )( ,m)(m,a)(a,y)(y, )( ,m)(m,o)(o,n)(n,e)(e,y)(y, )( ,o)(o,r)(r, )( ,s)(s,e)(e,x)(x, )( ,s)(s,t)(t,e)(e,t)(t,e)(e, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,f)(f,i)(i,l)(l,m)(m, )( ,b)(b,e)(e, )( ,r)(r,o)(o,l)(l,e)(e, )( ,f)(f,l)(l,i)(i,n)(n,s)(s, )( ,o)(o,f)(f, )( ,a)(a,n)(n,t)(t,i)\n",
      "chars: fyntority to experptions may money or sex stete of the film be role flins of ant\n",
      "bigrams: (d,w)(r,p)(p,e)(e,r)(r,t)(t,s)(s, )( ,o)(o,f)(f,t)(t,e)(e,n)(n, )( ,s)(s,o)(o,c)(c,i)(i,a)(a, )( ,e)(e,c)(c,o)(o,n)(n,o)(o,m)(m,y)(y, )( ,k)(k,a)(a,r)(r,i)(i,t)(t,a)(a,n)(n,s)(s, )( ,a)(a,l)(l,t)(t,o)(o,l)(l,i)(i,s)(s,i)(i,o)(o, )( ,w)(w,i)(i,l)(l,l)(l,i)(i,t)(t,a)(a,n)(n,c)(c,e)(e, )( ,t)(t,h)(h,e)(e, )( ,t)(t,h)(h,e)(e, )( ,p)(p, )( ,y)(y,e)(e,d)(d,g)(g,e)(e,n)(n,g)(g, )( ,o)(o,n)(n, )( ,t)(t,h)\n",
      "chars: drperts often socia economy karitans altolisio willitance the the p yedgeng on t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5100: 1.516960 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5200: 1.517624 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.08\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5300: 1.503491 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5400: 1.481007 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5500: 1.502585 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5600: 1.505600 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5700: 1.499465 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5800: 1.492936 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5900: 1.502381 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6000: 1.473320 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "================================================================================\n",
      "bigrams: (m,p)(p,h)(h,i)(i,c)(c, )( ,t)(t,o)(o, )( ,c)(c,r)(r,o)(o,s)(s,e)(e, )( ,t)(t,i)(i,m)(m,e)(e, )( ,h)(h,i)(i,s)(s, )( ,e)(e,v)(v,e)(e,n)(n,s)(s, )( ,o)(o,f)(f, )( ,k)(k,a)(a,s)(s,i)(i,c)(c, )( ,a)(a,n)(n,d)(d, )( ,t)(t,h)(h,e)(e, )( ,a)(a,p)(p,i)(i,l)(l,i)(i,d)(d,s)(s, )( ,u)(u,n)(n,i)(i,s)(s,o)(o,n)(n,a)(a,l)(l, )( ,c)(c,o)(o,n)(n,t)(t,r)(r,o)(o,l)(l,s)(s, )( ,t)(t,h)(h,e)(e,m)(m, )( ,w)(w,i)(i,l)\n",
      "chars: mphic to crose time his evens of kasic and the apilids unisonal controls them wi\n",
      "bigrams: (q,k)( ,q)(q,u)(u,e)(e, )( ,p)(p, )( ,a)(a,n)(n, )( ,u)(u,n)(n,d)(d,e)(e,r)(r,e)(e,d)(d, )( ,i)(i,n)(n,j)(j,u)(u,s)(s,e)(e, )( ,o)(o,f)(f, )( ,m)(m,u)(u,s)(s,l)(l,i)(i,s)(s,h)(h, )( ,t)(t,h)(h,r)(r,e)(e,e)(e, )( ,e)(j,e)(e, )( ,i)(i,s)(s, )( ,n)(n,o)(o,t)(t, )( ,a)(a,r)(r,e)(e, )( ,r)(r,e)(e,m)(m,o)(o,v)(v,e)(e,m)(m,e)(e,n)(n,c)(c,e)(e, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e, )( ,i)(i,t)(t,u)(u,r)(r,s)\n",
      "chars: q que p an undered injuse of muslish three je is not are removemence in the itur\n",
      "bigrams: (q,o)(l,e)(e,s)(s, )( ,s)(s,e)(e,x)(x,o)(o,n)(n,t)(t, )( ,u)(u,n)(n,f)(f,e)(e,e)(e,n)(n, )( ,c)(c,o)(o,i)(i,n)(n, )( ,v)(v,i)(i,d)(d,e)(e,o)(o,v)(v,e)(e,r)(r, )( ,a)(a,n)(n,d)(d, )( ,h)(h,e)(e, )( ,o)(o,t)(t,h)(h,e)(e,r)(r, )( ,a)(a,c)(c,c)(c,e)(e,p)(p,t)(t,e)(e,d)(d, )( ,f)(f,o)(o,u)(u,g)(r,t)(t,a)(a,l)(l, )( ,c)(c,o)(o,n)(n,t)(t,i)(i,n)(n,u)(u,e)(e,s)(s, )( ,t)(t,h)(h,i)(i,n)(n,g)(g, )( ,o)(o,f)\n",
      "chars: qles sexont unfeen coin videover and he other accepted fourtal continues thing o\n",
      "bigrams: (c,o)(o,r)(r,e)(e, )( ,s)(s,e)(e,x)(x,a)(a,l)(l, )( ,t)(t,w)(w,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,d)(d,o)(o,m)(m,a)(a,j)(j,t)(y,p)(p,h)(h,y)(y, )( ,b)(b,e)(e,l)(l,o)(o,n)(n,s)(s, )( ,a)(a,r)(r,e)(e, )( ,u)(u,s)(s,e)(e, )( ,p)(p,a)(a,y)(y,s)(s, )( ,g)(g,r)(r,e)(e,a)(a,s)(s,u)(u,e)(e, )( ,a)(a,n)(n,d)(d, )( ,o)(o,t)(t,h)(h,e)(e,r)(r, )( ,n)(n,e)(e,w)(w, )( ,m)(m,u)(u,s)(s,i)(i,c)(c, )( ,t)(t,o)(o, )\n",
      "chars: core sexal two zero domajyphy belons are use pays greasue and other new music to\n",
      "bigrams: (s,i)(i, )( ,t)(t,h)(h,e)(e, )( ,r)(r,a)(a,s)(s,u)(u,c)(c,r)(r,a)(a,l)(l, )( ,c)(c,e)(e,r)(r,m)(m,o)(o,r)(r,l)(l,e)(e,d)(d, )( ,a)(a,l)(l,r)(r,y)(y,l)(l,u)(u,e)(e, )( ,a)(a,a)(a,r)(r,e)(e, )( ,g)(g,r)(r,e)(e,a)(a,t)(t,e)(e,d)(d, )( ,b)(b,y)(y, )( ,t)(t,h)(h,e)(e, )( ,o)(o,b)(b,s)(s,e)(e,c)(c,t)(t, )( ,c)(c,o)(o,u)(u,r)(r,e)(e, )( ,a)(a,n)(n,d)(d, )( ,a)(a,c)(c,t)(t,i)(i,m)(m,i)(i,t)(t,c)(c,h)(h, )\n",
      "chars: si the rasucral cermorled alrylue aare greated by the obsect coure and actimitch\n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6100: 1.473022 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6200: 1.460691 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6300: 1.461541 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6400: 1.485533 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6500: 1.492548 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.18\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6600: 1.513906 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6700: 1.520055 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6800: 1.534612 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6900: 1.484611 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.21\n",
      "Average loss at step 7000: 1.495289 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "================================================================================\n",
      "bigrams: (z,q)(i,h)(h,i)(i,c)(c,k)(k, )( ,o)(o,f)(f,f)(f, )( ,s)(s,l)(l,a)(a,v)(v,i)(i,n)(n,g)(g, )( ,t)(t,o)(o, )( ,c)(c,o)(o,n)(n,t)(t,a)(a,n)(n,t)(t,i)(i,o)(o,n)(n,s)(s, )( ,h)(h,a)(a,p)(p,p)(p,o)(o,s)(s, )( ,e)(e,s)(s,h)(h,o)(o,r)(r,t)(t,e)(e,d)(d, )( ,t)(t,o)(o, )( ,i)(i,n)(n,f)(f,i)(i,r)(r,m)(m,i)(i,n)(n,g)(g, )( ,t)(t,o)(o, )( ,a)(a,m)(m,e)(e,r)(r,o)(o,n)(n,i)(i,s)(s,m)(m, )( ,c)(c,a)(a,n)(n, )( ,b)\n",
      "chars: zihick off slaving to contantions happos eshorted to infirming to ameronism can \n",
      "bigrams: (h,h)(n,r)(r,o)(o,u)(e,l)(l, )( ,t)(t,h)(h,a)(a,t)(t, )( ,t)(t,i)(i,m)(m,e)(e, )( ,t)(t,h)(h,e)(e, )( ,h)(h,e)(e, )( ,o)(o,s)(s,e)(e,s)(s, )( ,t)(t,h)(h,e)(e,o)(o,u)(u,p)(p,e)(e,d)(d, )( ,p)(p,r)(r,o)(o,t)(t,r)(r,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,u)(u,n)(n,r)(r,o)(o,a)(a,l)(l,t)(t,e)(e,d)(d, )( ,i)(i,n)(n,t)(t,o)(o, )( ,t)(t,h)(h,e)(e, )( ,d)(d,i)(i,g)(g,i)(i,n)(n,s)(s, )( ,o)(o,f)(f, )( ,t)(t,h)(h,a)\n",
      "chars: hnroel that time the he oses theouped protration unroalted into the digins of th\n",
      "bigrams: (n,u)(u,s)(s, )( ,u)(u,n)(n,d)(d,e)(e,r)(r, )( ,t)(t,h)(h,r)(r,e)(e,a)(a,t)(t,h)(h, )( ,a)(a,b)(b,o)(o,u)(u,t)(t, )( ,x)(k,a)(a,t)(t,i)(i,o)(o,n)(n,s)(s, )( ,t)(t,h)(h,e)(e, )( ,c)(c,u)(u,r)(r,r)(r,e)(e,c)(c,h)(h, )( ,m)(m,e)(e,b)(b,s)(s, )( ,w)(w,i)(i,r)(r,t)(t,h)(h,s)(s, )( ,b)(b,e)( ,s)(s,i)(i,x)(x, )( ,t)(t,h)(h,r)(r,e)(e,e)(e, )( ,f)(f,i)(i,v)(v,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,t)(t,w)\n",
      "chars: nus under threath about kations the currech mebs wirths b six three five seven t\n",
      "bigrams: (c,b)(z,i)(i,n)(n, )( ,e)(e,f)(f,f)(f,e)(e,c)(c,t)(t,s)(s, )( ,m)(m,a)(a,n)(n,t)(t,h)(h,e)(e,t)(t,h)(h, )( ,r)(r,o)(o,r)(r,d)(d, )( ,t)(t,o)(o, )( ,l)(l,o)(o,s)(s, )( ,p)(p,a)(a,r)(r,t)(t,i)(i,c)(c,l)(l,e)(e, )( ,h)(h,i)(i,m)(m, )( ,e)(e,t)(t,u)(u,r)(r,e)(e, )( ,t)(t,h)(h,a)(a,t)(t, )( ,s)(s,m)(m,a)(a,r)(r,k)(k,e)(e,d)(d, )( ,s)(s,e)(e,t)(t, )( ,o)(o,f)(f,t)(t,w)(w,o)(o, )( ,a)(a,d)(d,v)(v,a)(a,n)\n",
      "chars: czin effects mantheth rord to los particle him eture that smarked set oftwo adva\n",
      "bigrams: (e,q)(q,u)(u,e)(e,d)(d, )( ,p)(p,a)(a,n)(n,k)(k,a)(a, )( ,i)(i,l)(l,l)(l,y)(y,r)(r,i)(i,g)(g,n)(n,e)(e,d)(d, )( ,a)(a,n)(n,d)(d, )( ,d)(d,i)(i,e)(e,o)(o, )( ,a)(a,n)(n,d)(d, )( ,d)(d,e)(e,a)(a,t)(t,h)(h, )( ,m)(m,a)(a, )( ,r)(r,e)(e,b)(b, )( ,p)(p,r)(r,e)(e,s)(s,s)(s,a)(a,n)(n,t)(t, )( ,c)(c,a)(a,n)(n,n)(n,o)(o,t)(t,r)(r,i)(i,e)(e,s)(s,h)(h,m)(m,o)(o,u)(u,s)(s, )( ,f)(f,r)(r,e)(e,q)(q,u)(u,e)(e,n)\n",
      "chars: equed panka illyrigned and dieo and death ma reb pressant cannotrieshmous freque\n",
      "================================================================================\n",
      "Validation set perplexity: 4.20\n",
      "Wall time: 4min 9s\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "def exec_graph_bigram(graph):\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "      batches = bigram_train_batches.next()\n",
    "      feed_dict = dict()\n",
    "      for i in range(num_unrollings + 1):\n",
    "        feed_dict[train_data[i]] = batches[i]\n",
    "      _, l, predictions, lr = session.run(\n",
    "        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "      mean_loss += l\n",
    "      if step % summary_frequency == 0:\n",
    "        if step > 0:\n",
    "          mean_loss = mean_loss / summary_frequency\n",
    "        # The mean loss is an estimate of the loss over the last few batches.\n",
    "        print(\n",
    "          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "        mean_loss = 0\n",
    "        labels = np.concatenate(list(batches)[1:])\n",
    "        print('Minibatch perplexity: %.2f' % float(\n",
    "          np.exp(logprob(predictions, labels))))\n",
    "        if step % (summary_frequency * 10) == 0:\n",
    "          # Generate some samples.\n",
    "          print('=' * 80)\n",
    "          for _ in range(5):\n",
    "            feed = bigram_sample(bigram_random_distribution())\n",
    "            bigram_sentence = bigram_characters(feed)[0]\n",
    "            sentence = bigram_first_characters(feed)[0]\n",
    "            reset_sample_state.run()\n",
    "            for _ in range(79):\n",
    "              prediction = sample_prediction.eval({sample_input: feed})\n",
    "              feed = bigram_sample(prediction)\n",
    "              bigram_sentence += bigram_characters(feed)[0]\n",
    "              sentence += bigram_first_characters(feed)[0]\n",
    "            print('bigrams:', bigram_sentence)\n",
    "            print('chars:', sentence)\n",
    "          print('=' * 80)\n",
    "        # Measure validation set perplexity.\n",
    "        reset_sample_state.run()\n",
    "        valid_logprob = 0\n",
    "        for _ in range(valid_size):\n",
    "          b = bigram_valid_batches.next()\n",
    "          predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "          valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "        print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "          valid_logprob / valid_size)))\n",
    "        \n",
    "%time exec_graph_bigram(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
